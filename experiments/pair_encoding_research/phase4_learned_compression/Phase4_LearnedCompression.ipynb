{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üî¨ Phase 4: Learned Compression & Segmented Hypervectors\n",
        "\n",
        "## Expert Recommendations\n",
        "1. **Learned Compression:** 16384d ‚Üí 4096d via trainable W_pair\n",
        "2. **Segmented:** Split 4096d into [P:1536 | H:1536 | Interaction:1024]\n",
        "3. **Role-Filler:** Add role vectors for asymmetry\n",
        "\n",
        "## Goal\n",
        "Achieve ~65% accuracy in **4096d** (vs current 65.6% in 16384d)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers datasets\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"\\nüî¨ Phase 4: Learned Compression & Segmented\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use best encoder from Phase 2\n",
        "ENCODER_NAME = 'sentence-transformers/nli-mpnet-base-v2'\n",
        "print(f\"Loading {ENCODER_NAME}...\")\n",
        "encoder = SentenceTransformer(ENCODER_NAME)\n",
        "SEMANTIC_DIM = encoder.get_sentence_embedding_dimension()\n",
        "print(f\"‚úÖ Encoder loaded: {SEMANTIC_DIM}d\")"
      ],
      "metadata": {
        "id": "encoder"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNLI\n",
        "print(\"\\nLoading MNLI...\")\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 500\n",
        "\n",
        "train_data = dataset['train'].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
        "test_data = dataset['validation_matched'].select(range(TEST_SIZE))\n",
        "\n",
        "train_labels = np.array(train_data['label'])\n",
        "test_labels = np.array(test_data['label'])\n",
        "\n",
        "print(f\"‚úÖ Train: {TRAIN_SIZE}, Test: {TEST_SIZE}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-compute embeddings\n",
        "print(\"\\nüîÑ Computing embeddings...\")\n",
        "\n",
        "train_p = encoder.encode(list(train_data['premise']), show_progress_bar=True)\n",
        "train_h = encoder.encode(list(train_data['hypothesis']), show_progress_bar=True)\n",
        "test_p = encoder.encode(list(test_data['premise']), show_progress_bar=True)\n",
        "test_h = encoder.encode(list(test_data['hypothesis']), show_progress_bar=True)\n",
        "\n",
        "# Convert to torch\n",
        "train_p_t = torch.tensor(train_p, dtype=torch.float32)\n",
        "train_h_t = torch.tensor(train_h, dtype=torch.float32)\n",
        "test_p_t = torch.tensor(test_p, dtype=torch.float32)\n",
        "test_h_t = torch.tensor(test_h, dtype=torch.float32)\n",
        "train_labels_t = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_labels_t = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "print(f\"‚úÖ Embeddings: {train_p.shape}\")"
      ],
      "metadata": {
        "id": "embeddings"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Approach 1: Learned Compression (16384 ‚Üí 4096)\n",
        "---"
      ],
      "metadata": {
        "id": "approach1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnedCompressionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Two-Vector features (16384d) ‚Üí Learned compression ‚Üí 4096d ‚Üí Ternary ‚Üí MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, semantic_dim=768, hdc_dim=4096, hidden_dim=512, \n",
        "                 num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hdc_dim = hdc_dim\n",
        "        \n",
        "        # Random projection (frozen) - same as before\n",
        "        projection = torch.randn(semantic_dim, hdc_dim)\n",
        "        projection = projection / projection.norm(dim=0, keepdim=True)\n",
        "        self.register_buffer('projection', projection)\n",
        "        \n",
        "        # Learned compression: 16384 ‚Üí 4096\n",
        "        self.compressor = nn.Linear(hdc_dim * 4, hdc_dim, bias=False)\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hdc_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def ternary_quantize(self, x, threshold_factor=0.3):\n",
        "        \"\"\"Differentiable approximation of ternary quantization.\"\"\"\n",
        "        # During training: soft quantization\n",
        "        # During eval: hard quantization\n",
        "        if self.training:\n",
        "            # Use tanh as soft approximation\n",
        "            return torch.tanh(x * 3)  # Scale to make it more discrete-like\n",
        "        else:\n",
        "            # Hard ternary\n",
        "            std = x.std(dim=1, keepdim=True)\n",
        "            thr = threshold_factor * std\n",
        "            return torch.where(x > thr, torch.ones_like(x),\n",
        "                              torch.where(x < -thr, -torch.ones_like(x), \n",
        "                                         torch.zeros_like(x)))\n",
        "    \n",
        "    def forward(self, p_emb, h_emb, return_hdc=False):\n",
        "        # Project to HDC space\n",
        "        p_hdc = p_emb @ self.projection\n",
        "        h_hdc = h_emb @ self.projection\n",
        "        \n",
        "        # Two-vector features\n",
        "        diff = p_hdc - h_hdc\n",
        "        prod = p_hdc * h_hdc\n",
        "        features = torch.cat([p_hdc, h_hdc, diff, prod], dim=1)  # 16384d\n",
        "        \n",
        "        # Learned compression\n",
        "        compressed = self.compressor(features)  # 4096d\n",
        "        \n",
        "        # Ternary quantization\n",
        "        hdc_vec = self.ternary_quantize(compressed)\n",
        "        \n",
        "        if return_hdc:\n",
        "            return hdc_vec\n",
        "        \n",
        "        # Classify\n",
        "        logits = self.classifier(hdc_vec)\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ LearnedCompressionModel defined\")"
      ],
      "metadata": {
        "id": "learned_compression"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Approach 2: Segmented Hypervectors\n",
        "---"
      ],
      "metadata": {
        "id": "approach2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentedHDCModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Segmented approach: [P_block | H_block | Interaction_block]\n",
        "    No interference between P and H.\n",
        "    \"\"\"\n",
        "    def __init__(self, semantic_dim=768, total_dim=4096, hidden_dim=512,\n",
        "                 num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Segment sizes\n",
        "        self.p_dim = 1536\n",
        "        self.h_dim = 1536\n",
        "        self.i_dim = total_dim - self.p_dim - self.h_dim  # 1024\n",
        "        self.total_dim = total_dim\n",
        "        \n",
        "        # Separate projections for each block\n",
        "        # P and H use same projection (for comparability)\n",
        "        proj_main = torch.randn(semantic_dim, self.p_dim)\n",
        "        proj_main = proj_main / proj_main.norm(dim=0, keepdim=True)\n",
        "        self.register_buffer('proj_main', proj_main)\n",
        "        \n",
        "        # Interaction projection\n",
        "        proj_inter = torch.randn(semantic_dim, self.i_dim)\n",
        "        proj_inter = proj_inter / proj_inter.norm(dim=0, keepdim=True)\n",
        "        self.register_buffer('proj_inter', proj_inter)\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(total_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def ternary_quantize(self, x, threshold_factor=0.3):\n",
        "        if self.training:\n",
        "            return torch.tanh(x * 3)\n",
        "        else:\n",
        "            std = x.std(dim=1, keepdim=True)\n",
        "            thr = threshold_factor * std\n",
        "            return torch.where(x > thr, torch.ones_like(x),\n",
        "                              torch.where(x < -thr, -torch.ones_like(x), \n",
        "                                         torch.zeros_like(x)))\n",
        "    \n",
        "    def forward(self, p_emb, h_emb, return_hdc=False):\n",
        "        # Block 1: Premise\n",
        "        p_block = p_emb @ self.proj_main  # 1536d\n",
        "        \n",
        "        # Block 2: Hypothesis  \n",
        "        h_block = h_emb @ self.proj_main  # 1536d\n",
        "        \n",
        "        # Block 3: Interaction (computed in semantic space, then projected)\n",
        "        diff_emb = p_emb - h_emb\n",
        "        prod_emb = p_emb * h_emb\n",
        "        i_block = (diff_emb + prod_emb) @ self.proj_inter  # 1024d\n",
        "        \n",
        "        # Concatenate segments\n",
        "        full_vec = torch.cat([p_block, h_block, i_block], dim=1)  # 4096d\n",
        "        \n",
        "        # Ternary quantization\n",
        "        hdc_vec = self.ternary_quantize(full_vec)\n",
        "        \n",
        "        if return_hdc:\n",
        "            return hdc_vec\n",
        "        \n",
        "        # Classify\n",
        "        logits = self.classifier(hdc_vec)\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ SegmentedHDCModel defined\")"
      ],
      "metadata": {
        "id": "segmented"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Approach 3: Role-Filler Binding\n",
        "---"
      ],
      "metadata": {
        "id": "approach3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoleFillerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Role-Filler binding: P*R_prem + H*R_hyp + interaction\n",
        "    Proper HDC/VSA style.\n",
        "    \"\"\"\n",
        "    def __init__(self, semantic_dim=768, hdc_dim=4096, hidden_dim=512,\n",
        "                 num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hdc_dim = hdc_dim\n",
        "        \n",
        "        # Random projection\n",
        "        projection = torch.randn(semantic_dim, hdc_dim)\n",
        "        projection = projection / projection.norm(dim=0, keepdim=True)\n",
        "        self.register_buffer('projection', projection)\n",
        "        \n",
        "        # Role vectors (fixed, random ternary)\n",
        "        torch.manual_seed(42)\n",
        "        # Sparse ternary: ~25% -1, ~50% 0, ~25% +1\n",
        "        r_prem = torch.zeros(hdc_dim)\n",
        "        r_hyp = torch.zeros(hdc_dim)\n",
        "        \n",
        "        # Generate sparse ternary\n",
        "        for r in [r_prem, r_hyp]:\n",
        "            mask = torch.rand(hdc_dim)\n",
        "            r[mask < 0.25] = -1\n",
        "            r[mask > 0.75] = 1\n",
        "        \n",
        "        self.register_buffer('r_prem', r_prem)\n",
        "        self.register_buffer('r_hyp', r_hyp)\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hdc_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def ternary_quantize(self, x, threshold_factor=0.3):\n",
        "        if self.training:\n",
        "            return torch.tanh(x * 3)\n",
        "        else:\n",
        "            std = x.std(dim=1, keepdim=True)\n",
        "            thr = threshold_factor * std\n",
        "            return torch.where(x > thr, torch.ones_like(x),\n",
        "                              torch.where(x < -thr, -torch.ones_like(x), \n",
        "                                         torch.zeros_like(x)))\n",
        "    \n",
        "    def forward(self, p_emb, h_emb, return_hdc=False):\n",
        "        # Project to HDC\n",
        "        p_hdc = p_emb @ self.projection\n",
        "        h_hdc = h_emb @ self.projection\n",
        "        \n",
        "        # Bind with role vectors\n",
        "        p_bound = p_hdc * self.r_prem\n",
        "        h_bound = h_hdc * self.r_hyp\n",
        "        \n",
        "        # Bundle + difference for directionality\n",
        "        diff = p_hdc - h_hdc\n",
        "        pair_hv = p_bound + h_bound + diff\n",
        "        \n",
        "        # Ternary quantization\n",
        "        hdc_vec = self.ternary_quantize(pair_hv)\n",
        "        \n",
        "        if return_hdc:\n",
        "            return hdc_vec\n",
        "        \n",
        "        # Classify\n",
        "        logits = self.classifier(hdc_vec)\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ RoleFillerModel defined\")"
      ],
      "metadata": {
        "id": "role_filler"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Baseline: Two-Vector (16384d) for comparison\n",
        "---"
      ],
      "metadata": {
        "id": "baseline_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoVectorBaseline(nn.Module):\n",
        "    \"\"\"Baseline: Two-Vector with 16384d (from Phase 3).\"\"\"\n",
        "    def __init__(self, semantic_dim=768, hdc_dim=4096, hidden_dim=1024,\n",
        "                 num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hdc_dim = hdc_dim\n",
        "        \n",
        "        projection = torch.randn(semantic_dim, hdc_dim)\n",
        "        projection = projection / projection.norm(dim=0, keepdim=True)\n",
        "        self.register_buffer('projection', projection)\n",
        "        \n",
        "        # Larger classifier for 16384d input\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hdc_dim * 4, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def ternary_quantize(self, x, threshold_factor=0.3):\n",
        "        if self.training:\n",
        "            return torch.tanh(x * 3)\n",
        "        else:\n",
        "            std = x.std(dim=1, keepdim=True)\n",
        "            thr = threshold_factor * std\n",
        "            return torch.where(x > thr, torch.ones_like(x),\n",
        "                              torch.where(x < -thr, -torch.ones_like(x), \n",
        "                                         torch.zeros_like(x)))\n",
        "    \n",
        "    def forward(self, p_emb, h_emb, return_hdc=False):\n",
        "        p_hdc = p_emb @ self.projection\n",
        "        h_hdc = h_emb @ self.projection\n",
        "        \n",
        "        diff = p_hdc - h_hdc\n",
        "        prod = p_hdc * h_hdc\n",
        "        features = torch.cat([p_hdc, h_hdc, diff, prod], dim=1)  # 16384d\n",
        "        \n",
        "        hdc_vec = self.ternary_quantize(features)\n",
        "        \n",
        "        if return_hdc:\n",
        "            return hdc_vec\n",
        "        \n",
        "        logits = self.classifier(hdc_vec)\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ TwoVectorBaseline defined\")"
      ],
      "metadata": {
        "id": "baseline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, p_emb, h_emb, labels):\n",
        "        self.p_emb = p_emb\n",
        "        self.h_emb = h_emb\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.p_emb[idx], self.h_emb[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = PairDataset(train_p_t, train_h_t, train_labels_t)\n",
        "test_dataset = PairDataset(test_p_t, test_h_t, test_labels_t)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"‚úÖ DataLoaders ready\")"
      ],
      "metadata": {
        "id": "dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, num_epochs=30, lr=1e-3):\n",
        "    \"\"\"Train model and return best accuracy.\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    \n",
        "    best_acc = 0\n",
        "    history = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        for p_emb, h_emb, labels in train_loader:\n",
        "            p_emb = p_emb.to(device)\n",
        "            h_emb = h_emb.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(p_emb, h_emb)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for p_emb, h_emb, labels in test_loader:\n",
        "                p_emb = p_emb.to(device)\n",
        "                h_emb = h_emb.to(device)\n",
        "                \n",
        "                logits = model(p_emb, h_emb)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "        \n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        history.append(acc)\n",
        "        best_acc = max(best_acc, acc)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"   Epoch {epoch+1}: {acc:.1%}\")\n",
        "    \n",
        "    return best_acc, history"
      ],
      "metadata": {
        "id": "train_fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî¨ RUNNING PHASE 4 EXPERIMENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Models to test\n",
        "models = {\n",
        "    '0_baseline_16384d': (TwoVectorBaseline, 16384),\n",
        "    '1_learned_compression_4096d': (LearnedCompressionModel, 4096),\n",
        "    '2_segmented_4096d': (SegmentedHDCModel, 4096),\n",
        "    '3_role_filler_4096d': (RoleFillerModel, 4096),\n",
        "}\n",
        "\n",
        "for name, (ModelClass, dim) in models.items():\n",
        "    print(f\"\\nüìä Testing: {name} ({dim}d)\")\n",
        "    \n",
        "    model = ModelClass()\n",
        "    acc, history = train_model(model, train_loader, test_loader, num_epochs=30)\n",
        "    \n",
        "    results[name] = {\n",
        "        'accuracy': acc,\n",
        "        'dim': dim,\n",
        "        'history': history\n",
        "    }\n",
        "    \n",
        "    print(f\"   ‚úÖ Best: {acc:.1%}\")\n",
        "    \n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_acc = results['0_baseline_16384d']['accuracy']\n",
        "phase2_best = 0.638  # From Phase 2\n",
        "\n",
        "print(f\"\\n{'Approach':<35} {'Dim':<8} {'Accuracy':<10} {'vs Baseline'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "\n",
        "for name, data in sorted_results:\n",
        "    diff = (data['accuracy'] - baseline_acc) * 100\n",
        "    marker = \"üèÜ\" if data['accuracy'] == sorted_results[0][1]['accuracy'] else \"  \"\n",
        "    print(f\"{marker} {name:<33} {data['dim']:<8} {data['accuracy']:.1%}      {diff:+.1f}%\")\n",
        "\n",
        "print(f\"\\nüìà Phase 2 best (for reference): {phase2_best:.1%}\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key analysis\n",
        "best_name = sorted_results[0][0]\n",
        "best_acc = sorted_results[0][1]['accuracy']\n",
        "best_dim = sorted_results[0][1]['dim']\n",
        "\n",
        "# Find best 4096d approach\n",
        "best_4096 = max([(n, d) for n, d in results.items() if d['dim'] == 4096],\n",
        "                key=lambda x: x[1]['accuracy'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ KEY FINDINGS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüèÜ Overall best: {best_name}\")\n",
        "print(f\"   Accuracy: {best_acc:.1%}, Dim: {best_dim}\")\n",
        "\n",
        "print(f\"\\nüéØ Best 4096d approach: {best_4096[0]}\")\n",
        "print(f\"   Accuracy: {best_4096[1]['accuracy']:.1%}\")\n",
        "print(f\"   vs 16384d baseline: {(best_4096[1]['accuracy'] - baseline_acc)*100:+.1f}%\")\n",
        "\n",
        "if best_4096[1]['accuracy'] >= baseline_acc - 0.02:\n",
        "    print(f\"\\n‚úÖ SUCCESS: 4096d achieves similar accuracy to 16384d!\")\n",
        "    print(f\"   Can use 4x smaller vectors for protocol.\")\n",
        "    verdict = \"COMPRESSION_SUCCESS\"\n",
        "elif best_4096[1]['accuracy'] >= phase2_best:\n",
        "    print(f\"\\nüìà PARTIAL: 4096d better than Phase 2, but below 16384d baseline\")\n",
        "    verdict = \"PARTIAL_SUCCESS\"\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è 4096d approaches underperform\")\n",
        "    verdict = \"NEEDS_MORE_WORK\""
      ],
      "metadata": {
        "id": "analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart\n",
        "ax = axes[0]\n",
        "names = [n.replace('_', '\\n') for n, _ in sorted_results]\n",
        "accs = [d['accuracy'] for _, d in sorted_results]\n",
        "dims = [d['dim'] for _, d in sorted_results]\n",
        "colors = ['lightgreen' if d == 4096 else 'lightblue' for d in dims]\n",
        "\n",
        "bars = ax.bar(names, accs, color=colors, edgecolor='black')\n",
        "ax.axhline(y=baseline_acc, color='blue', linestyle='--', alpha=0.7, \n",
        "           label=f'16384d baseline ({baseline_acc:.1%})')\n",
        "ax.axhline(y=phase2_best, color='red', linestyle=':', alpha=0.7,\n",
        "           label=f'Phase 2 best ({phase2_best:.1%})')\n",
        "ax.axhline(y=0.33, color='gray', linestyle=':', alpha=0.3)\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Phase 4: Compression Approaches')\n",
        "ax.legend(loc='lower right')\n",
        "ax.set_ylim(0.3, max(accs) + 0.05)\n",
        "\n",
        "for bar, acc, dim in zip(bars, accs, dims):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "            f'{acc:.1%}\\n({dim}d)', ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Learning curves\n",
        "ax = axes[1]\n",
        "for name, data in results.items():\n",
        "    short_name = name.split('_')[1] if '_' in name else name\n",
        "    ax.plot(data['history'], label=f\"{short_name}: {data['accuracy']:.1%}\", linewidth=2)\n",
        "\n",
        "ax.axhline(y=baseline_acc, color='blue', linestyle='--', alpha=0.5)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Learning Curves')\n",
        "ax.legend(loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('phase4_compression_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "output = {\n",
        "    'experiment': 'Phase 4: Learned Compression & Segmented',\n",
        "    'dataset': 'MNLI',\n",
        "    'encoder': ENCODER_NAME,\n",
        "    'train_size': TRAIN_SIZE,\n",
        "    'test_size': TEST_SIZE,\n",
        "    'results': {k: {'accuracy': v['accuracy'], 'dim': v['dim']} \n",
        "                for k, v in results.items()},\n",
        "    'best_overall': best_name,\n",
        "    'best_4096d': best_4096[0],\n",
        "    'best_4096d_accuracy': best_4096[1]['accuracy'],\n",
        "    'baseline_16384d': baseline_acc,\n",
        "    'compression_loss': baseline_acc - best_4096[1]['accuracy'],\n",
        "    'verdict': verdict,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open('phase4_compression_results.json', 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Results saved!\")\n",
        "print(json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã CONCLUSIONS & NEXT STEPS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\"\"\n",
        "Phase 4 Results:\n",
        "- Baseline (16384d): {baseline_acc:.1%}\n",
        "- Best 4096d: {best_4096[1]['accuracy']:.1%} ({best_4096[0]})\n",
        "- Compression loss: {(baseline_acc - best_4096[1]['accuracy'])*100:.1f}%\n",
        "\n",
        "For Resonance Protocol:\n",
        "- If 4096d is acceptable: Use {best_4096[0]}\n",
        "- If need maximum accuracy: Use 16384d Two-Vector\n",
        "- Trade-off: {(baseline_acc - best_4096[1]['accuracy'])*100:.1f}% accuracy for 4x compression\n",
        "\n",
        "Remaining options to try:\n",
        "1. Learned sentence‚ÜíHDC projection (W_sent)\n",
        "2. Larger HDC dim (8192d) as middle ground\n",
        "3. More training data\n",
        "4. Fine-tune NLI encoder for HDC\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "conclusions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('phase4_compression_results.json')\n",
        "files.download('phase4_compression_results.png')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
