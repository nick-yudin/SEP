{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üî¨ Phase 2: NLI-Tuned Encoder\n",
        "\n",
        "## Finding from Phase 1\n",
        "Baseline A (float, no HDC) = 54.6% ‚Äî the encoder/features are the bottleneck!\n",
        "\n",
        "## Hypothesis\n",
        "NLI-tuned encoders should provide better relational features.\n",
        "\n",
        "## Encoders to Test\n",
        "| Encoder | Training | Size |\n",
        "|---------|----------|------|\n",
        "| all-MiniLM-L6-v2 (baseline) | General similarity | 384d |\n",
        "| nli-distilroberta-base-v2 | SNLI + MNLI | 768d |\n",
        "| nli-mpnet-base-v2 | AllNLI | 768d |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers datasets\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"\\nüî¨ Phase 2: NLI-Tuned Encoder\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNLI\n",
        "print(\"Loading MNLI...\")\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 500\n",
        "\n",
        "train_data = dataset['train'].shuffle(seed=42).select(range(TRAIN_SIZE))\n",
        "test_data = dataset['validation_matched'].select(range(TEST_SIZE))\n",
        "\n",
        "train_labels = np.array(train_data['label'])\n",
        "test_labels = np.array(test_data['label'])\n",
        "\n",
        "train_premises = list(train_data['premise'])\n",
        "train_hypotheses = list(train_data['hypothesis'])\n",
        "test_premises = list(test_data['premise'])\n",
        "test_hypotheses = list(test_data['hypothesis'])\n",
        "\n",
        "print(f\"‚úÖ Train: {TRAIN_SIZE}, Test: {TEST_SIZE}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoders to test\n",
        "ENCODERS = {\n",
        "    'MiniLM (baseline)': 'all-MiniLM-L6-v2',\n",
        "    'NLI-DistilRoBERTa': 'sentence-transformers/nli-distilroberta-base-v2',\n",
        "    'NLI-MPNet': 'sentence-transformers/nli-mpnet-base-v2',\n",
        "}\n",
        "\n",
        "print(\"üìã Encoders to test:\")\n",
        "for name, model_name in ENCODERS.items():\n",
        "    print(f\"   {name}: {model_name}\")"
      ],
      "metadata": {
        "id": "encoders_list"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and Model\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=512, num_classes=3, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pair_features(p, h):\n",
        "    \"\"\"Create pair features: [P, H, P-H, P*H]\"\"\"\n",
        "    return np.concatenate([p, h, p - h, p * h], axis=1)\n",
        "\n",
        "def random_projection(features, dim=4096, seed=42):\n",
        "    \"\"\"Random projection to higher dimension.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    proj = np.random.randn(features.shape[1], dim).astype(np.float32)\n",
        "    proj /= np.linalg.norm(proj, axis=0, keepdims=True)\n",
        "    return features @ proj\n",
        "\n",
        "def ternary_quantize(features):\n",
        "    \"\"\"Quantize to {-1, 0, +1}.\"\"\"\n",
        "    thr = 0.3 * np.std(features, axis=1, keepdims=True)\n",
        "    return np.where(features > thr, 1,\n",
        "                    np.where(features < -thr, -1, 0)).astype(np.float32)\n",
        "\n",
        "def train_and_evaluate(train_features, train_labels, test_features, test_labels,\n",
        "                       input_dim, num_epochs=25, lr=1e-3):\n",
        "    \"\"\"Train MLP and return best accuracy.\"\"\"\n",
        "    \n",
        "    train_dataset = SimpleDataset(train_features, train_labels)\n",
        "    test_dataset = SimpleDataset(test_features, test_labels)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "    \n",
        "    model = MLPClassifier(input_dim=input_dim).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    \n",
        "    best_acc = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(features), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        with torch.no_grad():\n",
        "            for features, _ in test_loader:\n",
        "                features = features.to(device)\n",
        "                preds = torch.argmax(model(features), dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "        \n",
        "        acc = accuracy_score(test_labels, all_preds)\n",
        "        best_acc = max(best_acc, acc)\n",
        "    \n",
        "    return best_acc"
      ],
      "metadata": {
        "id": "functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî¨ TESTING DIFFERENT ENCODERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "HDC_DIM = 4096\n",
        "results = {}\n",
        "\n",
        "for encoder_name, model_name in ENCODERS.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä Testing: {encoder_name}\")\n",
        "    print(f\"   Model: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load encoder\n",
        "    print(\"   Loading encoder...\")\n",
        "    encoder = SentenceTransformer(model_name)\n",
        "    emb_dim = encoder.get_sentence_embedding_dimension()\n",
        "    print(f\"   Embedding dim: {emb_dim}\")\n",
        "    \n",
        "    # Encode\n",
        "    print(\"   Encoding sentences...\")\n",
        "    train_p = encoder.encode(train_premises, show_progress_bar=True)\n",
        "    train_h = encoder.encode(train_hypotheses, show_progress_bar=True)\n",
        "    test_p = encoder.encode(test_premises, show_progress_bar=True)\n",
        "    test_h = encoder.encode(test_hypotheses, show_progress_bar=True)\n",
        "    \n",
        "    # Create pair features\n",
        "    train_features = make_pair_features(train_p, train_h)\n",
        "    test_features = make_pair_features(test_p, test_h)\n",
        "    feature_dim = train_features.shape[1]\n",
        "    print(f\"   Feature dim: {feature_dim}\")\n",
        "    \n",
        "    # Test all 4 configurations\n",
        "    encoder_results = {}\n",
        "    \n",
        "    # A: Float baseline\n",
        "    print(\"\\n   Testing A: Float (no HDC)...\")\n",
        "    acc_A = train_and_evaluate(train_features, train_labels, \n",
        "                                test_features, test_labels, feature_dim)\n",
        "    encoder_results['A_float'] = acc_A\n",
        "    print(f\"   ‚úÖ A: {acc_A:.1%}\")\n",
        "    \n",
        "    # B: Float + Projection\n",
        "    print(\"   Testing B: Float + Projection...\")\n",
        "    train_proj = random_projection(train_features, HDC_DIM)\n",
        "    test_proj = random_projection(test_features, HDC_DIM)\n",
        "    acc_B = train_and_evaluate(train_proj, train_labels,\n",
        "                                test_proj, test_labels, HDC_DIM)\n",
        "    encoder_results['B_projected'] = acc_B\n",
        "    print(f\"   ‚úÖ B: {acc_B:.1%}\")\n",
        "    \n",
        "    # C: Ternary only\n",
        "    print(\"   Testing C: Ternary (no projection)...\")\n",
        "    train_tern = ternary_quantize(train_features)\n",
        "    test_tern = ternary_quantize(test_features)\n",
        "    acc_C = train_and_evaluate(train_tern, train_labels,\n",
        "                                test_tern, test_labels, feature_dim)\n",
        "    encoder_results['C_ternary'] = acc_C\n",
        "    print(f\"   ‚úÖ C: {acc_C:.1%}\")\n",
        "    \n",
        "    # D: Full HDC\n",
        "    print(\"   Testing D: Full HDC (projection + ternary)...\")\n",
        "    train_hdc = ternary_quantize(train_proj)\n",
        "    test_hdc = ternary_quantize(test_proj)\n",
        "    acc_D = train_and_evaluate(train_hdc, train_labels,\n",
        "                                test_hdc, test_labels, HDC_DIM)\n",
        "    encoder_results['D_full_hdc'] = acc_D\n",
        "    print(f\"   ‚úÖ D: {acc_D:.1%}\")\n",
        "    \n",
        "    encoder_results['embedding_dim'] = emb_dim\n",
        "    encoder_results['feature_dim'] = feature_dim\n",
        "    results[encoder_name] = encoder_results\n",
        "    \n",
        "    # Clean up\n",
        "    del encoder\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "run_experiments"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create comparison table\n",
        "print(f\"\\n{'Encoder':<25} {'Emb':<6} {'A:Float':<10} {'B:Proj':<10} {'C:Tern':<10} {'D:HDC':<10}\")\n",
        "print(\"-\" * 75)\n",
        "\n",
        "for encoder_name, data in results.items():\n",
        "    print(f\"{encoder_name:<25} {data['embedding_dim']:<6} \"\n",
        "          f\"{data['A_float']:.1%}      {data['B_projected']:.1%}      \"\n",
        "          f\"{data['C_ternary']:.1%}      {data['D_full_hdc']:.1%}\")"
      ],
      "metadata": {
        "id": "show_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate improvements\n",
        "baseline_A = results['MiniLM (baseline)']['A_float']\n",
        "baseline_D = results['MiniLM (baseline)']['D_full_hdc']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà IMPROVEMENT OVER BASELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n{'Encoder':<25} {'A vs baseline':<15} {'D vs baseline'}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for encoder_name, data in results.items():\n",
        "    imp_A = (data['A_float'] - baseline_A) * 100\n",
        "    imp_D = (data['D_full_hdc'] - baseline_D) * 100\n",
        "    print(f\"{encoder_name:<25} {imp_A:+.1f}%          {imp_D:+.1f}%\")"
      ],
      "metadata": {
        "id": "improvements"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find best encoder\n",
        "best_encoder = max(results.keys(), key=lambda k: results[k]['A_float'])\n",
        "best_A = results[best_encoder]['A_float']\n",
        "best_D = results[best_encoder]['D_full_hdc']\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ CONCLUSIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüèÜ Best encoder: {best_encoder}\")\n",
        "print(f\"   Float baseline (A): {best_A:.1%}\")\n",
        "print(f\"   Full HDC (D): {best_D:.1%}\")\n",
        "\n",
        "if best_A > 0.65:\n",
        "    print(f\"\\n‚úÖ NLI encoder significantly improved baseline!\")\n",
        "    print(f\"   Improvement: +{(best_A - baseline_A)*100:.1f}%\")\n",
        "    print(f\"   ‚Üí Encoder was the main bottleneck\")\n",
        "    verdict = \"NLI_ENCODER_HELPS\"\n",
        "elif best_A > 0.58:\n",
        "    print(f\"\\nüìà Moderate improvement with NLI encoder\")\n",
        "    print(f\"   Improvement: +{(best_A - baseline_A)*100:.1f}%\")\n",
        "    print(f\"   ‚Üí Encoder helps, but other factors also matter\")\n",
        "    verdict = \"PARTIAL_IMPROVEMENT\"\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è NLI encoder did not help much\")\n",
        "    print(f\"   Improvement: +{(best_A - baseline_A)*100:.1f}%\")\n",
        "    print(f\"   ‚Üí Need different approach (two-vector, architecture change)\")\n",
        "    verdict = \"NEED_DIFFERENT_APPROACH\"\n",
        "\n",
        "# HDC overhead\n",
        "hdc_overhead = (best_A - best_D) * 100\n",
        "print(f\"\\nüìâ HDC overhead (A‚ÜíD): {hdc_overhead:.1f}%\")"
      ],
      "metadata": {
        "id": "conclusions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Comparison bars\n",
        "ax = axes[0]\n",
        "encoder_names = list(results.keys())\n",
        "x = np.arange(len(encoder_names))\n",
        "width = 0.2\n",
        "\n",
        "configs = ['A_float', 'B_projected', 'C_ternary', 'D_full_hdc']\n",
        "config_labels = ['A: Float', 'B: +Proj', 'C: Ternary', 'D: HDC']\n",
        "colors = ['green', 'lightblue', 'orange', 'lightcoral']\n",
        "\n",
        "for i, (cfg, label, color) in enumerate(zip(configs, config_labels, colors)):\n",
        "    values = [results[enc][cfg] for enc in encoder_names]\n",
        "    bars = ax.bar(x + i*width, values, width, label=label, color=color, edgecolor='black')\n",
        "\n",
        "ax.axhline(y=0.33, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Encoder Comparison: All Configurations')\n",
        "ax.set_xticks(x + 1.5*width)\n",
        "ax.set_xticklabels([n.replace(' ', '\\n') for n in encoder_names])\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_ylim(0.3, max([results[e]['A_float'] for e in encoder_names]) + 0.1)\n",
        "\n",
        "# Float baseline comparison\n",
        "ax = axes[1]\n",
        "float_accs = [results[enc]['A_float'] for enc in encoder_names]\n",
        "hdc_accs = [results[enc]['D_full_hdc'] for enc in encoder_names]\n",
        "\n",
        "x = np.arange(len(encoder_names))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, float_accs, width, label='A: Float (ceiling)', color='green', edgecolor='black')\n",
        "bars2 = ax.bar(x + width/2, hdc_accs, width, label='D: Full HDC', color='lightcoral', edgecolor='black')\n",
        "\n",
        "ax.axhline(y=0.33, color='gray', linestyle='--', alpha=0.5)\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Float Ceiling vs Full HDC')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([n.replace(' ', '\\n') for n in encoder_names])\n",
        "ax.legend()\n",
        "\n",
        "for bar, acc in zip(bars1, float_accs):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "            f'{acc:.1%}', ha='center', fontweight='bold', fontsize=10)\n",
        "for bar, acc in zip(bars2, hdc_accs):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "            f'{acc:.1%}', ha='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('phase2_encoder_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "output = {\n",
        "    'experiment': 'Phase 2: NLI-Tuned Encoder',\n",
        "    'dataset': 'MNLI',\n",
        "    'train_size': TRAIN_SIZE,\n",
        "    'test_size': TEST_SIZE,\n",
        "    'hdc_dim': HDC_DIM,\n",
        "    'results': results,\n",
        "    'best_encoder': best_encoder,\n",
        "    'best_float_accuracy': best_A,\n",
        "    'best_hdc_accuracy': best_D,\n",
        "    'baseline_improvement': float(best_A - baseline_A),\n",
        "    'hdc_overhead': float(best_A - best_D),\n",
        "    'verdict': verdict,\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open('phase2_encoder_results.json', 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Results saved!\")\n",
        "print(json.dumps(output, indent=2))"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìã NEXT STEPS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if verdict == \"NLI_ENCODER_HELPS\":\n",
        "    print(\"\"\"\n",
        "‚úÖ NLI encoder is the solution!\n",
        "\n",
        "Next steps:\n",
        "1. Use best NLI encoder as default for pair tasks\n",
        "2. Test on full MNLI dataset\n",
        "3. Try Project-First-Bind-Later with NLI encoder\n",
        "    \"\"\")\n",
        "elif verdict == \"PARTIAL_IMPROVEMENT\":\n",
        "    print(\"\"\"\n",
        "üìà NLI encoder helps but not enough.\n",
        "\n",
        "Next steps:\n",
        "1. Try two-vector approach (separate HDC for P and H)\n",
        "2. Try Project-First-Bind-Later architecture\n",
        "3. Combine NLI encoder with better architecture\n",
        "    \"\"\")\n",
        "else:\n",
        "    print(\"\"\"\n",
        "‚ö†Ô∏è Need fundamentally different approach.\n",
        "\n",
        "Next steps:\n",
        "1. Try two-vector approach (P_hdc + H_hdc ‚Üí MLP)\n",
        "2. Try role-filler binding (VSA-style)\n",
        "3. Consider learned projections\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "next_steps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('phase2_encoder_results.json')\n",
        "files.download('phase2_encoder_comparison.png')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
