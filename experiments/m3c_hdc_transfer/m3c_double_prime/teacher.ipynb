{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  M3câ€³: TRUE Semantic Knowledge Transfer â€” Teacher\n",
        "\n",
        "## The Key Difference from M3câ€²\n",
        "\n",
        "**M3câ€² (old):** Student received (text, label) pairs. Embeddings were unused.\n",
        "\n",
        "**M3câ€³ (new):** Student receives (HDC_vector, label) pairs. **No text at all!**\n",
        "\n",
        "```\n",
        "M3câ€²: Teacher â†’ (text, label) â†’ Student learns textâ†’label\n",
        "M3câ€³: Teacher â†’ (HDC_meaning, label) â†’ Student learns meaningâ†’label\n",
        "```\n",
        "\n",
        "**This is true semantic transfer:** The Student never sees the original text.\n",
        "It only sees the MEANING encoded as an HDC vector.\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "```\n",
        "1. Teacher (DistilBERT) trains on text â†’ learns sentiment\n",
        "2. For each high-confidence example:\n",
        "   - Encode text â†’ HDC vector (using HDC encoder)\n",
        "   - Store (HDC_vector, label) â€” NO TEXT!\n",
        "3. Upload HDC knowledge packet to Firebase\n",
        "4. Student downloads packet\n",
        "5. Student trains: HDC_vector â†’ label (never sees text)\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate\n",
        "!pip install -q firebase-admin\n",
        "!pip install -q sentence-transformers\n",
        "print(\"âœ… Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import base64\n",
        "\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NODE_ID = \"teacher_hdc\"\n",
        "print(f\"\\nðŸ§  This is {NODE_ID.upper()} â€” True HDC Semantic Transfer\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”· HDC Encoder â€” The Heart of Semantic Transfer\n",
        "\n",
        "This is where meaning becomes algebra."
      ],
      "metadata": {
        "id": "hdc_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HDCEncoder:\n",
        "    \"\"\"\n",
        "    Hyperdimensional Computing Encoder for Semantic Representation.\n",
        "    \n",
        "    Converts text â†’ high-dimensional binary/ternary vector\n",
        "    that captures MEANING, not surface form.\n",
        "    \n",
        "    Key properties:\n",
        "    - Compositional: meaning of phrase = combination of word meanings\n",
        "    - Noise-tolerant: similar meanings â†’ similar vectors\n",
        "    - Architecture-independent: any model can read HDC vectors\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, dim=4096, seed=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: Dimensionality of HDC vectors (higher = more capacity)\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.dim = dim\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        \n",
        "        # Use SentenceTransformer to get initial semantic representation\n",
        "        # Then project to HDC space\n",
        "        self.semantic_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.semantic_dim = 384  # MiniLM output dimension\n",
        "        \n",
        "        # Random projection matrix: semantic space â†’ HDC space\n",
        "        # This preserves similarity structure (Johnson-Lindenstrauss)\n",
        "        self.projection = self.rng.randn(self.semantic_dim, self.dim).astype(np.float32)\n",
        "        self.projection /= np.linalg.norm(self.projection, axis=0, keepdims=True)\n",
        "        \n",
        "        print(f\"âœ… HDC Encoder initialized\")\n",
        "        print(f\"   Semantic dim: {self.semantic_dim}\")\n",
        "        print(f\"   HDC dim: {self.dim}\")\n",
        "    \n",
        "    def encode(self, texts, quantize=True):\n",
        "        \"\"\"\n",
        "        Encode text(s) to HDC vectors.\n",
        "        \n",
        "        Args:\n",
        "            texts: string or list of strings\n",
        "            quantize: if True, return ternary {-1, 0, +1} vectors\n",
        "        \n",
        "        Returns:\n",
        "            HDC vectors as numpy array [n_texts, dim]\n",
        "        \"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        \n",
        "        # Step 1: Get semantic embeddings\n",
        "        semantic_vecs = self.semantic_encoder.encode(\n",
        "            texts, \n",
        "            convert_to_numpy=True,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "        \n",
        "        # Step 2: Project to HDC space\n",
        "        hdc_vecs = semantic_vecs @ self.projection\n",
        "        \n",
        "        # Step 3: Quantize to ternary (optional but recommended)\n",
        "        if quantize:\n",
        "            # Ternary quantization with threshold\n",
        "            threshold = 0.3 * np.std(hdc_vecs, axis=1, keepdims=True)\n",
        "            hdc_vecs = np.where(hdc_vecs > threshold, 1,\n",
        "                       np.where(hdc_vecs < -threshold, -1, 0)).astype(np.int8)\n",
        "        \n",
        "        return hdc_vecs\n",
        "    \n",
        "    def similarity(self, vec1, vec2):\n",
        "        \"\"\"Cosine similarity between HDC vectors.\"\"\"\n",
        "        if vec1.ndim == 1:\n",
        "            vec1 = vec1.reshape(1, -1)\n",
        "        if vec2.ndim == 1:\n",
        "            vec2 = vec2.reshape(1, -1)\n",
        "        \n",
        "        # For ternary vectors, dot product / dim is a good similarity\n",
        "        return (vec1 @ vec2.T) / self.dim\n",
        "\n",
        "# Initialize HDC Encoder\n",
        "HDC_DIM = 4096  # Can experiment with 1024, 2048, 4096, 8192\n",
        "hdc_encoder = HDCEncoder(dim=HDC_DIM, seed=42)"
      ],
      "metadata": {
        "id": "hdc_encoder"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test HDC Encoder\n",
        "print(\"\\nðŸ§ª Testing HDC Encoder...\")\n",
        "\n",
        "test_sentences = [\n",
        "    \"This movie is great!\",\n",
        "    \"This film is wonderful!\",  # Similar meaning\n",
        "    \"This movie is terrible.\",   # Opposite meaning\n",
        "    \"The weather is sunny.\",     # Unrelated\n",
        "]\n",
        "\n",
        "hdc_vecs = hdc_encoder.encode(test_sentences)\n",
        "print(f\"\\nHDC vector shape: {hdc_vecs.shape}\")\n",
        "print(f\"HDC vector dtype: {hdc_vecs.dtype}\")\n",
        "print(f\"Sparsity (% zeros): {(hdc_vecs == 0).mean() * 100:.1f}%\")\n",
        "\n",
        "print(\"\\nðŸ“Š Similarity matrix:\")\n",
        "print(f\"{'':30} | great | wonderful | terrible | weather\")\n",
        "print(\"-\" * 75)\n",
        "for i, sent in enumerate(test_sentences):\n",
        "    sims = [hdc_encoder.similarity(hdc_vecs[i], hdc_vecs[j])[0,0] for j in range(4)]\n",
        "    print(f\"{sent:30} | {sims[0]:5.2f} | {sims[1]:9.2f} | {sims[2]:8.2f} | {sims[3]:7.2f}\")\n",
        "\n",
        "print(\"\\nâœ… Similar meanings â†’ high similarity\")\n",
        "print(\"âœ… Opposite meanings â†’ lower similarity\")\n",
        "print(\"âœ… Unrelated â†’ near zero similarity\")"
      ],
      "metadata": {
        "id": "test_hdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Firebase credentials\n",
        "FIREBASE_CREDENTIALS = {\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"resonance-m3\",\n",
        "  \"private_key_id\": \"124e2cb57b123eefac08b105c14afa647d3f90e6\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDMy0nOWX9JaQWk\\nRS3Mz+l5ib8wiwORkJ/rK1ekJoFwaQA3LKM9F1LhAIIRd/cv7X8cfmK5S84g5Yv7\\nrYRMqBVTqItfwC8bET6i6Sf4ooxwwFuIO6qMBUCju3Yqf+ri5gP6GLdKE2cIdGZe\\nSefMIMe8qql609Dnn6BZ7QrKjgEhgd/byYcLuuFoKNTKYSIw++TqmaRBATEOpI4c\\nstCtx42rhp3Rq03ZpNfGDo67Ua0dCzto1NKkScCfUlgLd3v5OrQuU/37a9vgsoEE\\nmcnWxEpjj8Edd4O+KKt0jbduDcMLbKoe42Csrh11VdIRxtJhn0W3dWcb63ZIN1cV\\nBdlR2VsDAgMBAAECggEAG5NsDBsaUU+Q8OkCcnJfOnxagDYtHtazPJQI0xOpmPH/\\n6MNb9QfvowMnKyRU5ovmR8yaN9/bO6wXyKXkC1EUE2sTNADJga2KxPOm7DXcZKT3\\ng5qf1lwxXgVfXfhSL9ylFcdqNtFHWGjEDVeKR371Jj8ctGtSf+iLS6XoLr98VinM\\na7hvEQBy7Y8wUcugrnv08uzvGPDR+MjcLPZ6YIotDgATklbbv2bZf27ZvhZb666J\\nCS/KhqrS9F/6gTxGjxkJ7EGr14apXcLYnyl4WnlnorDOF87U9eCUh/QPSJmNqrbQ\\nP42ZgJkjgclPMnuLHJFtck0LAP6JbJG2KGkn2BMEIQKBgQDss52wA58/73nlVPZX\\nS9jfmFZX79zCqUr8aipeZ9FKbG2yJ/N3MkW6UdeJfPJpxtYyyzKsZy14sLx5Tiii\\nJ3VenhFJm/9CnnyMdGKl4khHpCxQm3whAMVQJmngTAWdK/jSxG7sXvXsBcOYJEr5\\n9nTUqbhx9cQZ5wX0CBpTShKTewKBgQDdfbSWdA6ZCf+ptdvRxtoCqmDKYIAdH1rH\\nbaUvXfJPTzGhO0uNhdGyOe8om67xRu9U8DqOBZD38l5IES5dtxf7Yiqau3KHh/t6\\n73/kJ6UfskhcR6pKkFv1AH5BoxS2BgCChMZO4B87bU3Z5IILvjbbd/7oK8aNqfRo\\nMPhHx16cGQKBgQCzr9raHrXK3GnlzWzAcmStwMBzOzSUVd4F5jZJungoDk3r++YY\\nK4LBYZXE2qRP5lD++EB8nkrnnwtE7y9rgZbZABfRkGnj0dps6YFlunTyZc/6VT6S\\n8znWkYK4ch2k5hebMOGf8KqxxOJp4ctFHHIuarUJVe/LVhSv32LUrZuSdwKBgQCU\\n6BosvEHEKbC0TAtI8UNIX3tLE62N5rKFOPXA7owlPbzEBLU6pCjihYh/6Iqos2Pb\\n2tXC/YME7vDryoE9iAabftfdxv+sloM/lxyIKw8cTCPRxmx6TKtF/9riDd2ysj5N\\ngS5BgPP+Y/He931mn68JxZaeSC/otcYW9jU1LJgyoQKBgDd8YwxP5YkcRnJzIWAe\\n6CMhYOTNxAKnVwSBNctSG4ztzRa+wRSp0G8YNJCmwKKN7xXfJQGLVD8Shq7cfDY2\\nU+0s3jEzbiIqY9W+FQy3dNsfxxR5WK9isG9fWSFfSg4ichXtTGim2eyKxoxlak2+\\nyvvxzpjUtPKMdW2Sp0HkuS13\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"firebase-adminsdk-fbsvc@resonance-m3.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"103650910750904165580\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/firebase-adminsdk-fbsvc%40resonance-m3.iam.gserviceaccount.com\",\n",
        "  \"universe_domain\": \"googleapis.com\"\n",
        "}\n",
        "\n",
        "FIREBASE_DATABASE_URL = \"https://resonance-m3-default-rtdb.europe-west1.firebasedatabase.app\"\n",
        "\n",
        "# Initialize Firebase\n",
        "if not firebase_admin._apps:\n",
        "    cred = credentials.Certificate(FIREBASE_CREDENTIALS)\n",
        "    firebase_admin.initialize_app(cred, {\n",
        "        'databaseURL': FIREBASE_DATABASE_URL\n",
        "    })\n",
        "print(\"âœ… Firebase initialized\")"
      ],
      "metadata": {
        "id": "firebase"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Train Teacher Model"
      ],
      "metadata": {
        "id": "teacher_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2\n",
        ").to(device)\n",
        "\n",
        "print(f\"âœ… Teacher model loaded: {MODEL_NAME}\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading SST-2 dataset...\")\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "\n",
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 500\n",
        "\n",
        "train_texts = dataset['train']['sentence'][:TRAIN_SIZE]\n",
        "train_labels = dataset['train']['label'][:TRAIN_SIZE]\n",
        "\n",
        "test_texts = dataset['validation']['sentence'][:TEST_SIZE]\n",
        "test_labels = dataset['validation']['label'][:TEST_SIZE]\n",
        "\n",
        "print(f\"âœ… Train: {len(train_texts)}, Test: {len(test_texts)}\")"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.encodings = tokenizer(\n",
        "            list(texts),\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        self.labels = torch.tensor(list(labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.encodings['input_ids'][idx],\n",
        "            'attention_mask': self.encodings['attention_mask'][idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "print(f\"âœ… Datasets created\")"
      ],
      "metadata": {
        "id": "create_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions_with_confidence(model, texts, tokenizer, batch_size=32):\n",
        "    \"\"\"Get predictions and confidence scores.\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting\"):\n",
        "        batch_texts = list(texts[i:i+batch_size])\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = F.softmax(outputs.logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "            confs = probs.max(dim=1).values\n",
        "            \n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            confidences.extend(confs.cpu().numpy())\n",
        "\n",
        "    return np.array(predictions), np.array(confidences)\n",
        "\n",
        "# Measure BEFORE training\n",
        "print(\"ðŸ“Š Measuring Teacher BEFORE training...\")\n",
        "preds_before, _ = get_predictions_with_confidence(model, test_texts, tokenizer)\n",
        "acc_before = accuracy_score(test_labels, preds_before)\n",
        "print(f\"âœ… Teacher accuracy BEFORE: {acc_before:.4f}\")"
      ],
      "metadata": {
        "id": "before_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./teacher_output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ‹ï¸ Training Teacher...\")\n",
        "trainer.train()\n",
        "print(\"âœ… Training complete!\")"
      ],
      "metadata": {
        "id": "train_teacher"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure AFTER training\n",
        "print(\"ðŸ“Š Measuring Teacher AFTER training...\")\n",
        "preds_after, confs_after = get_predictions_with_confidence(model, test_texts, tokenizer)\n",
        "acc_after = accuracy_score(test_labels, preds_after)\n",
        "print(f\"âœ… Teacher accuracy AFTER: {acc_after:.4f}\")\n",
        "print(f\"ðŸ“ˆ Improvement: +{acc_after - acc_before:.4f}\")"
      ],
      "metadata": {
        "id": "after_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Create HDC Knowledge Packet\n",
        "\n",
        "**THIS IS THE KEY DIFFERENCE!**\n",
        "\n",
        "We encode each example as an HDC vector. The Student will ONLY see these vectors, NOT the original text."
      ],
      "metadata": {
        "id": "hdc_packet_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ§  CREATING HDC KNOWLEDGE PACKET\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nâš ï¸  KEY: Student will ONLY see HDC vectors, NOT text!\\n\")\n",
        "\n",
        "# Get predictions on training data\n",
        "train_preds, train_confs = get_predictions_with_confidence(model, train_texts, tokenizer)\n",
        "\n",
        "# Select high-confidence correct predictions\n",
        "CONFIDENCE_THRESHOLD = 0.90\n",
        "MAX_EXAMPLES = 500\n",
        "\n",
        "hdc_knowledge = []\n",
        "\n",
        "for i in tqdm(range(len(train_texts)), desc=\"Encoding to HDC\"):\n",
        "    pred = train_preds[i]\n",
        "    conf = train_confs[i]\n",
        "    true_label = train_labels[i]\n",
        "    \n",
        "    # Only include high-confidence correct predictions\n",
        "    if pred == true_label and conf > CONFIDENCE_THRESHOLD:\n",
        "        # Encode text to HDC vector\n",
        "        hdc_vec = hdc_encoder.encode(train_texts[i], quantize=True)[0]\n",
        "        \n",
        "        hdc_knowledge.append({\n",
        "            'hdc_vector': hdc_vec.tolist(),  # Ternary vector\n",
        "            'label': int(true_label),\n",
        "            'confidence': float(conf),\n",
        "            # NO TEXT STORED!\n",
        "        })\n",
        "        \n",
        "        if len(hdc_knowledge) >= MAX_EXAMPLES:\n",
        "            break\n",
        "\n",
        "print(f\"\\nâœ… Created {len(hdc_knowledge)} HDC knowledge examples\")\n",
        "print(f\"   Each example: HDC vector ({HDC_DIM}d) + label\")\n",
        "print(f\"   NO TEXT in packet!\")"
      ],
      "metadata": {
        "id": "create_hdc_packet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the knowledge packet\n",
        "labels = [ex['label'] for ex in hdc_knowledge]\n",
        "print(f\"\\nðŸ“Š Knowledge Packet Statistics:\")\n",
        "print(f\"   Total examples: {len(hdc_knowledge)}\")\n",
        "print(f\"   Positive (1): {sum(labels)}\")\n",
        "print(f\"   Negative (0): {len(labels) - sum(labels)}\")\n",
        "print(f\"   HDC vector dim: {len(hdc_knowledge[0]['hdc_vector'])}\")\n",
        "\n",
        "# Check sparsity\n",
        "sample_vec = np.array(hdc_knowledge[0]['hdc_vector'])\n",
        "print(f\"   Sparsity (% zeros): {(sample_vec == 0).mean() * 100:.1f}%\")\n",
        "print(f\"   Values: {np.unique(sample_vec)}\")"
      ],
      "metadata": {
        "id": "analyze_packet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress for upload\n",
        "def compress_hdc_packet(knowledge):\n",
        "    \"\"\"\n",
        "    Compress HDC vectors for efficient transmission.\n",
        "    Ternary values {-1, 0, 1} â†’ 2 bits each â†’ pack 4 per byte.\n",
        "    \"\"\"\n",
        "    compressed = []\n",
        "    \n",
        "    for ex in knowledge:\n",
        "        vec = np.array(ex['hdc_vector'], dtype=np.int8)\n",
        "        \n",
        "        # Map: -1 â†’ 0, 0 â†’ 1, 1 â†’ 2 (fits in 2 bits)\n",
        "        mapped = (vec + 1).astype(np.uint8)\n",
        "        \n",
        "        # Pack 4 values per byte\n",
        "        packed = np.zeros(len(vec) // 4, dtype=np.uint8)\n",
        "        for j in range(4):\n",
        "            packed |= (mapped[j::4] << (j * 2))\n",
        "        \n",
        "        compressed.append({\n",
        "            'hdc_packed': base64.b64encode(packed.tobytes()).decode('ascii'),\n",
        "            'label': ex['label'],\n",
        "            'confidence': ex['confidence']\n",
        "        })\n",
        "    \n",
        "    return compressed\n",
        "\n",
        "compressed_knowledge = compress_hdc_packet(hdc_knowledge)\n",
        "\n",
        "# Calculate sizes\n",
        "raw_size = len(hdc_knowledge) * HDC_DIM * 1  # 1 byte per int8\n",
        "compressed_size = sum(len(ex['hdc_packed']) for ex in compressed_knowledge)\n",
        "\n",
        "print(f\"\\nðŸ“¦ Compression:\")\n",
        "print(f\"   Raw size: {raw_size / 1024:.1f} KB\")\n",
        "print(f\"   Compressed size: {compressed_size / 1024:.1f} KB\")\n",
        "print(f\"   Compression ratio: {raw_size / compressed_size:.1f}Ã—\")"
      ],
      "metadata": {
        "id": "compress_packet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Upload HDC Knowledge Packet to Firebase"
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ“¤ Uploading HDC Knowledge Packet to Firebase...\")\n",
        "\n",
        "# Prepare metadata\n",
        "knowledge_packet = {\n",
        "    'experiment': 'M3c_double_prime',\n",
        "    'description': 'TRUE HDC Semantic Transfer - Student sees ONLY HDC vectors',\n",
        "    'teacher_model': MODEL_NAME,\n",
        "    'teacher_accuracy_before': float(acc_before),\n",
        "    'teacher_accuracy_after': float(acc_after),\n",
        "    'teacher_improvement': float(acc_after - acc_before),\n",
        "    'hdc_dim': HDC_DIM,\n",
        "    'num_examples': len(compressed_knowledge),\n",
        "    'examples': compressed_knowledge,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'note': 'NO TEXT IN THIS PACKET - Only HDC vectors and labels'\n",
        "}\n",
        "\n",
        "# Upload\n",
        "ref = db.reference('sep_m3c_hdc/knowledge_packet')\n",
        "ref.set(knowledge_packet)\n",
        "\n",
        "# Signal ready\n",
        "ref = db.reference('sep_m3c_hdc/status')\n",
        "ref.set({\n",
        "    'teacher_ready': True,\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'hdc_dim': HDC_DIM,\n",
        "    'num_examples': len(compressed_knowledge),\n",
        "    'teacher_accuracy': float(acc_after)\n",
        "})\n",
        "\n",
        "print(f\"\\nâœ… HDC Knowledge Packet uploaded!\")\n",
        "print(f\"\\nðŸ“‹ Summary:\")\n",
        "print(f\"   Teacher: {MODEL_NAME}\")\n",
        "print(f\"   Accuracy: {acc_before:.1%} â†’ {acc_after:.1%} (+{(acc_after-acc_before)*100:.1f}%)\")\n",
        "print(f\"   HDC examples: {len(compressed_knowledge)}\")\n",
        "print(f\"   HDC dimension: {HDC_DIM}\")\n",
        "print(f\"   Packet size: {compressed_size / 1024:.1f} KB\")\n",
        "print(f\"\\nâš ï¸  REMEMBER: No text in packet! Only HDC vectors!\")"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results locally\n",
        "results = {\n",
        "    'phase': 'M3c_double_prime',\n",
        "    'node': 'teacher_hdc',\n",
        "    'experiment': 'TRUE HDC Semantic Transfer - Teacher',\n",
        "    'model': MODEL_NAME,\n",
        "    'train_samples': TRAIN_SIZE,\n",
        "    'test_samples': TEST_SIZE,\n",
        "    'accuracy_before': float(acc_before),\n",
        "    'accuracy_after': float(acc_after),\n",
        "    'improvement': float(acc_after - acc_before),\n",
        "    'hdc_dim': HDC_DIM,\n",
        "    'hdc_examples': len(compressed_knowledge),\n",
        "    'packet_size_kb': float(compressed_size / 1024),\n",
        "    'timestamp': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open('m3c_hdc_teacher_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ“ TEACHER NODE COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nðŸ“¤ HDC Knowledge Packet is ready for Student!\")\n",
        "print(\"\\nâš ï¸  Student will receive ONLY:\")\n",
        "print(\"    - HDC vectors (meaning)\")\n",
        "print(\"    - Labels\")\n",
        "print(\"    - NO text!\")\n",
        "print(\"\\nðŸš€ Run Student notebook now!\")"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('m3c_hdc_teacher_results.json')"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
