# M3c HDC Transfer Optimization Experiments

## Overview

Systematic optimization of HDC-based semantic knowledge transfer. Starting from M3c‚Ä≥ baseline (59.2% transfer efficiency), we tested single improvements to find the optimal configuration.

## Results Summary

| Experiment | Change | Transfer Efficiency | vs Baseline | Verdict |
|------------|--------|---------------------|-------------|---------|
| **M3c‚Ä≥** | Baseline (500 ex, hard labels, 4096d) | 59.2% | ‚Äî | Baseline |
| **M3c‚Å¥** | Soft labels | 69.3% | +10.1% | ‚úÖ Works |
| **M3c‚Åµ** | 2000 examples | **73.2%** | **+14.0%** | üèÜ **Best** |
| **M3c‚Å∂** | 10000d HDC | 54.5% | -4.7% | ‚ùå Worse |
| **M3c‚Å∑** | Soft + 2000 ex | 68.7% | +9.5% | ‚ö†Ô∏è No synergy |
| **M3c‚Å∏** | 5000 examples | 58.6% | -0.6% | ‚ùå Diminishing returns |

## Key Findings

### What Works ‚úÖ

1. **More training examples (up to 2000)**
   - 500 ‚Üí 2000 examples: +14% efficiency
   - Sweet spot found at ~2000 examples
   - Diminishing returns beyond 2000

2. **Soft labels (teacher probabilities)**
   - Hard [1,0] ‚Üí Soft [0.92, 0.08]: +10.1% efficiency
   - KL divergence loss provides richer signal

### What Doesn't Work ‚ùå

1. **Larger HDC dimension**
   - 4096d ‚Üí 10000d: -4.7% efficiency
   - More parameters need more data to train
   - 4096d is sufficient for this task

2. **Combining improvements**
   - Soft labels + More data ‚â† additive effect
   - Combined: 68.7% < More data alone: 73.2%
   - Soft labels may "blur" signal when data is abundant

3. **Excessive data (5000 examples)**
   - 2000 ‚Üí 5000: -14.6% efficiency
   - Student starts at higher baseline, less room for improvement
   - Metric artifact, not real degradation

## Best Configuration

```
M3c‚Åµ (2000 examples)
‚îú‚îÄ‚îÄ HDC dimension: 4096
‚îú‚îÄ‚îÄ Training examples: 2000
‚îú‚îÄ‚îÄ Labels: Hard
‚îú‚îÄ‚îÄ Student: 3-layer MLP (2.2M params)
‚îú‚îÄ‚îÄ Transfer efficiency: 73.2%
‚îî‚îÄ‚îÄ Student accuracy: 76.8%
```

## Methodology

Each experiment changed **one variable** from baseline:

```
Baseline (M3c‚Ä≥):
- 500 high-confidence examples
- Hard labels [1, 0] or [0, 1]
- HDC dimension: 4096
- 3-layer MLP student
- CrossEntropyLoss
- Adam + CosineAnnealing
```

## Directory Structure

```
m3c_optimization/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ baseline/           # M3c‚Ä≥ (59.2%)
‚îú‚îÄ‚îÄ soft_labels/        # M3c‚Å¥ (69.3%)
‚îú‚îÄ‚îÄ more_examples_2000/ # M3c‚Åµ (73.2%) ‚Üê BEST
‚îú‚îÄ‚îÄ larger_hdc/         # M3c‚Å∂ (54.5%)
‚îú‚îÄ‚îÄ combined/           # M3c‚Å∑ (68.7%)
‚îî‚îÄ‚îÄ more_examples_5000/ # M3c‚Å∏ (58.6%)
```

## Lessons Learned

1. **Simple improvements often work best**
   - More data > clever techniques

2. **One change at a time**
   - Combining improvements doesn't guarantee synergy

3. **Sweet spots exist**
   - More is not always better (data, dimensions)

4. **Transfer efficiency vs accuracy**
   - Different metrics can tell different stories
   - M3c‚Å∏ has highest accuracy (77.4%) but worst efficiency (58.6%)

## Next Steps

- [ ] Commit to SEP repository
- [ ] Try learned projection (trainable HDC encoder)
- [ ] Test on different tasks (not just SST-2)
- [ ] Hardware PoC with optimal configuration
