\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}

% Custom colors
\definecolor{sepblue}{RGB}{41, 128, 185}
\definecolor{sepgreen}{RGB}{39, 174, 96}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=sepblue,
    citecolor=sepblue,
    urlcolor=sepblue
}

% Title
\title{Cross-Architecture Knowledge Transfer\\via Hyperdimensional Computing}
\author{
    Nikolay Yudin\\
    \texttt{1@seprotocol.ai}\\
    Semantic Event Protocol
}
\date{December 2025}

\begin{document}

\maketitle

%==============================================================================
\begin{abstract}
%==============================================================================
We investigate whether semantic knowledge can be transferred between neural network architectures using Hyperdimensional Computing (HDC) with ternary quantization. Through experiments on sentiment and topic classification tasks, we observe that cross-architecture transfer achieves 94--99\% efficiency relative to student model ceilings when using contrastive alignment. In our experimental setup, we find that teacher model size did not correlate with transfer quality---a 66M parameter model achieved comparable or better results than models up to 14B parameters. These observations suggest that, at least for the tasks and configurations we tested, distributed AI systems may achieve effective knowledge sharing without requiring massive centralized models.
\end{abstract}

%==============================================================================
\section{Introduction}
%==============================================================================

The current AI landscape is dominated by centralized, ever-larger models. Training state-of-the-art systems requires hundreds of millions of dollars in compute, creating barriers to entry and concentrating power in a handful of organizations. This centralization poses risks: single points of failure, vendor lock-in, and limited accessibility for resource-constrained deployments.

An alternative paradigm envisions distributed networks of smaller, specialized models that share knowledge through efficient semantic representations. For this vision to succeed, we need mechanisms for \textit{cross-architecture knowledge transfer}---the ability to transmit learned knowledge from one model architecture to another without sharing raw data or model weights.

Hyperdimensional Computing (HDC) offers a promising foundation for such transfer. HDC represents information as high-dimensional vectors where similarity corresponds to semantic relatedness. These vectors can be aggressively quantized to ternary values $\{-1, 0, +1\}$ while preserving semantic relationships, enabling 32$\times$ compression over float32 representations.

In this paper, we investigate two questions:
\begin{enumerate}
    \item Can knowledge be effectively transferred between different neural network architectures through HDC representations?
    \item Does using a larger, more capable ``teacher'' model improve transfer quality?
\end{enumerate}

Regarding the second question, our experiments with SST-2 sentiment classification did not show improvement from larger teachers. In fact, the smallest model (66M parameters) achieved the best results in our setup. While this does not rule out benefits from larger teachers in other configurations or tasks, it suggests that architectural compatibility may be more important than raw model capacity for HDC-based transfer.

\subsection{Contributions}

\begin{itemize}
    \item Systematic evaluation of cross-architecture transfer via HDC, observing 94--99\% efficiency in our experiments
    \item Comparison of alignment methods, with contrastive learning showing the best results among tested approaches
    \item Empirical evidence that, in our setup, architectural compatibility appeared more important than model capacity for transfer
    \item Practical observations for distributed AI systems using HDC-based knowledge sharing
\end{itemize}

%==============================================================================
\section{Background}
%==============================================================================

\subsection{Hyperdimensional Computing}

Hyperdimensional Computing~\cite{kanerva2009hyperdimensional} represents information as high-dimensional vectors (typically 1,000--10,000 dimensions) where:
\begin{itemize}
    \item Random vectors are nearly orthogonal with high probability
    \item Similarity is measured by cosine distance
    \item Arithmetic operations (addition, multiplication) have semantic interpretations
\end{itemize}

A key property of HDC is robustness to quantization. Vectors can be reduced to ternary values:
\begin{equation}
    q(x_i) = \begin{cases}
        +1 & \text{if } x_i > \tau \cdot \sigma \\
        -1 & \text{if } x_i < -\tau \cdot \sigma \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $\sigma$ is the standard deviation and $\tau$ is a threshold (typically 0.3). This achieves 32$\times$ compression while preserving semantic relationships.

\subsection{Knowledge Distillation}

Traditional knowledge distillation~\cite{hinton2015distilling} transfers knowledge from a teacher to a student by training the student to match the teacher's output distributions. This requires:
\begin{itemize}
    \item Access to teacher model weights or outputs
    \item Shared training data
    \item Compatible output spaces
\end{itemize}

Our approach differs by transferring through an intermediate HDC representation, requiring only a small set of ``anchor'' examples for alignment calibration.

\subsection{Related Work}

Our work intersects three research areas: HDC for NLP, knowledge distillation, and cross-lingual embedding alignment.

\paragraph{HDC for NLP}
Hyperdimensional computing has been applied to various NLP tasks. Najafabadi et al.~\cite{najafabadi2016hdc} demonstrated text classification using character n-gram encoding, achieving 94\% accuracy on news categorization. Kleyko et al.~\cite{kleyko2020hyperembed} showed that HDC-based embeddings achieve competitive F1 scores with significant memory and speed improvements over conventional n-gram statistics. However, these works focus on single-model scenarios---training and inference occur within the same HDC encoding. Our work extends this to \textit{cross-architecture} transfer, where teacher and student use different underlying models.

\paragraph{Knowledge Distillation}
Standard knowledge distillation~\cite{hinton2015distilling} transfers knowledge by training students to match teacher output distributions. This requires access to teacher outputs and shared training procedures. Cross-architecture distillation~\cite{sanh2019distilbert} has produced models like DistilBERT, but still requires joint training. Our approach differs fundamentally: we transfer through a shared HDC representation space, requiring only anchor examples for alignment calibration, not shared training.

\paragraph{Cross-Lingual Embedding Alignment}
The problem of aligning embedding spaces across languages shares structural similarities with our cross-architecture setting. MUSE~\cite{conneau2017word} uses Procrustes alignment with bilingual dictionaries (supervised) or adversarial training (unsupervised) to map embeddings between languages. Schuster et al.~\cite{schuster2019crosslingual} extended this to contextual embeddings (ELMo) by aligning context-independent anchor spaces. Our contrastive alignment method draws inspiration from these approaches but addresses a different challenge: aligning representations from \textit{architecturally different} models (encoders vs decoders) rather than \textit{linguistically different} corpora.

%==============================================================================
\section{Methods}
%==============================================================================

\subsection{HDC Encoding Pipeline}

Given text input, our encoding pipeline consists of:

\begin{enumerate}
    \item \textbf{Embedding extraction}: Extract hidden states from a transformer model, applying mean pooling over tokens to obtain a fixed-size vector $e \in \mathbb{R}^{d}$
    
    \item \textbf{Random projection}: Project to HDC space via $h = eP$ where $P \in \mathbb{R}^{d \times D}$ is a random projection matrix with normalized columns
    
    \item \textbf{Ternary quantization}: Apply threshold-based quantization to obtain $t \in \{-1, 0, +1\}^D$
\end{enumerate}

\subsection{Alignment Methods}

Cross-architecture transfer requires aligning embedding spaces. We evaluate four approaches:

\paragraph{No Alignment (Baseline)} Use the same random projection seed for both models, relying on implicit structure similarity.

\paragraph{Procrustes} Find orthogonal transformation $R$ minimizing $\|T_a R - S_a\|_F$ where $T_a, S_a$ are teacher and student anchor embeddings.

\paragraph{Canonical Correlation Analysis (CCA)} Find projections maximizing correlation between teacher and student representations.

\paragraph{Contrastive Alignment} Learn neural projections $f_T, f_S$ minimizing:
\begin{equation}
    \mathcal{L} = \underbrace{(1 - \text{sim}(f_T(t_i), f_S(s_i)))}_{\text{positive pairs}} + \underbrace{\max(0, \text{sim}(f_T(t_i), f_S(s_j)) - m)}_{\text{negative pairs}}
\end{equation}
where $m$ is a margin hyperparameter.

\subsection{Transfer Efficiency Metric}

We define transfer efficiency as:
\begin{equation}
    \eta = \frac{\text{Acc}_{\text{transfer}}}{\text{Acc}_{\text{ceiling}}}
\end{equation}
where $\text{Acc}_{\text{ceiling}}$ is achieved when training and testing on the student's own HDC embeddings (no transfer). This normalizes for differences in student model quality.

%==============================================================================
\section{Experiments}
%==============================================================================

\subsection{Setup}

\paragraph{Models} We evaluate five transformer models (Table~\ref{tab:models}).

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Type} & \textbf{Params} & \textbf{Embed Dim} \\
\midrule
DistilBERT & Encoder & 66M & 768 \\
GPT-2 & Decoder & 124M & 768 \\
RoBERTa & Encoder & 125M & 768 \\
Llama 3.1 & Decoder & 8B & 4096 \\
Qwen 2.5 & Decoder & 14B & 5120 \\
\bottomrule
\end{tabular}
\caption{Models evaluated in experiments.}
\label{tab:models}
\end{table}

\paragraph{Datasets}
\begin{itemize}
    \item \textbf{SST-2}: Binary sentiment classification (2 classes)
    \item \textbf{AG News}: Topic classification (4 classes: World, Sports, Business, Technology)
\end{itemize}

\paragraph{Configuration} HDC dimension 4096, threshold $\tau=0.3$, 500 anchor samples, 3 random seeds per experiment.

\subsection{Experiment 1: Complete Transfer Pipeline}

We systematically evaluate each component of the transfer pipeline.

\subsubsection{HDC Quantization Cost}

Table~\ref{tab:hdc_cost} shows accuracy loss from ternary quantization:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Fine-tuned} & \textbf{HDC 8192d} & \textbf{Loss} \\
\midrule
DistilBERT & 87.8\% & 81.8\% & $-6.0$\% \\
GPT-2 & 88.8\% & 78.2\% & $-10.6$\% \\
\bottomrule
\end{tabular}
\caption{Cost of ternary quantization on SST-2.}
\label{tab:hdc_cost}
\end{table}

In these experiments, DistilBERT embeddings showed smaller degradation from quantization compared to GPT-2.

\subsubsection{Alignment Methods}

Figure~\ref{fig:alignment} compares alignment methods for DistilBERT$\rightarrow$GPT-2 transfer. In our tests, contrastive alignment achieved 96\% efficiency, outperforming other methods we evaluated.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/stage2_alignment_methods.png}
\caption{Alignment method comparison. Contrastive learning achieved highest efficiency across anchor sizes in our experiments.}
\label{fig:alignment}
\end{figure}

\subsubsection{Model Pairs}

Table~\ref{tab:pairs} shows that transfer worked in both directions with similar efficiency:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Transfer} & \textbf{Accuracy} & \textbf{Efficiency} \\
\midrule
DistilBERT $\rightarrow$ GPT-2 & 75.1\% & 94.1\% \\
GPT-2 $\rightarrow$ DistilBERT & 75.5\% & 93.7\% \\
DistilBERT $\rightarrow$ RoBERTa & 80.9\% & 96.1\% \\
RoBERTa $\rightarrow$ DistilBERT & 77.9\% & 99.8\% \\
\bottomrule
\end{tabular}
\caption{Bidirectional transfer results on SST-2.}
\label{tab:pairs}
\end{table}

Same-family transfers (BERT $\leftrightarrow$ BERT variants) showed higher efficiency in our experiments.

\subsubsection{Task Generalization}

We tested transfer on two classification tasks with different complexity:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Classes} & \textbf{Accuracy} & \textbf{Efficiency} \\
\midrule
SST-2 & 2 & 75\% & 95\% \\
AG News & 4 & 86\% & 100\% \\
\bottomrule
\end{tabular}
\caption{Transfer performance across datasets (DistilBERT $\rightarrow$ GPT-2).}
\label{tab:tasks}
\end{table}

Both single-sentence classification tasks showed high transfer efficiency. The 4-class task (AG News) achieved results comparable to the student ceiling, suggesting that HDC transfer scales to multi-class problems in this setting.

\subsection{Experiment 2: Teacher Size Study}

We tested whether larger teachers improve transfer quality in our setup.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Teacher} & \textbf{Params} & \textbf{Accuracy} & \textbf{Efficiency} \\
\midrule
DistilBERT & 66M & 78.9\% & 98.8\% \\
GPT-2 & 124M & 74.3\% & 93.9\% \\
Llama 3.1 & 8B & 77.7\% & 98.3\% \\
Qwen 2.5 & 14B & 75.6\% & 96.0\% \\
\bottomrule
\end{tabular}
\caption{Teacher size study results. Student: DistilBERT. Task: SST-2.}
\label{tab:strong_teacher}
\end{table}

\paragraph{Observation} In this experimental setup, larger models did not show improved transfer quality. The smallest model (DistilBERT) achieved the best results among tested teachers. However, we note several caveats:

\begin{itemize}
    \item This was tested on a single task (SST-2) and dataset
    \item Different alignment configurations might yield different results
    \item Larger models might show benefits on more complex tasks
    \item The alignment bottleneck (projecting to 2048d) may disproportionately affect higher-dimensional embeddings
\end{itemize}

%==============================================================================
\section{Discussion}
%==============================================================================

\subsection{Possible Explanations for Teacher Size Results}

Several factors may explain why larger teachers did not improve transfer in our setup:

\paragraph{Architectural Compatibility} DistilBERT$\rightarrow$DistilBERT transfer achieved alignment similarity of 0.995 versus 0.96 for cross-architecture pairs. Models from the same family may share embedding geometry that facilitates alignment.

\paragraph{Alignment Bottleneck} All embeddings were projected to a shared 2048-dimensional space. Higher-dimensional embeddings (Qwen: 5120d) lose more information during this projection. Different projection strategies might yield different results.

\paragraph{Embedding Structure} BERT-family encoders and GPT-family decoders have different representational structures. This architectural difference may create overhead that offsets benefits from larger model capacity.

\subsection{Implications and Limitations}

Our results suggest that, for the specific tasks and configurations tested, effective semantic transfer can be achieved without requiring massive teacher models. However, we emphasize several limitations:

\begin{itemize}
    \item Experiments were limited to single-sentence classification tasks
    \item Sentence-pair tasks (e.g., NLI, paraphrase detection) were not evaluated and may require different encoding strategies
    \item We tested a specific alignment method and hyperparameter configuration
    \item Results may differ with other HDC dimensions, anchor sizes, or alignment architectures
    \item Generative and reasoning tasks may show different patterns
\end{itemize}

These findings are preliminary and should be validated across broader experimental conditions before drawing general conclusions about distributed AI architectures.

%==============================================================================
\section{Conclusion}
%==============================================================================

We present experiments on cross-architecture knowledge transfer via HDC, observing 94--99\% efficiency when using contrastive alignment on classification tasks. In our setup, alignment quality appeared more important than teacher model size---smaller models with compatible architectures achieved better results than larger models with different architectures.

These observations, while preliminary, suggest directions for further research in distributed AI systems. If validated across broader conditions, the findings would imply that networks of small, specialized agents could achieve efficient semantic transfer through HDC representations, potentially enabling more accessible and distributed AI deployments.

Future work should investigate:
\begin{itemize}
    \item Whether these patterns hold for more complex tasks
    \item Alternative alignment methods for cross-family transfer
    \item HDC encoding strategies for sentence-pair tasks (NLI, paraphrase detection) which may require capturing inter-sentence relationships
    \item Scaling behavior with different anchor sizes and HDC dimensions
\end{itemize}

%==============================================================================
\section*{Acknowledgments}
%==============================================================================

This work is part of the Semantic Event Protocol (SEP) project. Code and data available at \url{https://github.com/nick-yudin/SEP}.

%==============================================================================
\bibliographystyle{plain}
\bibliography{references}
%==============================================================================

\end{document}
